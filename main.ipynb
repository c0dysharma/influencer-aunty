{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb808da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3136c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [{'user': 'alex',\n",
    "  'message': 'Have you looked into ClawDBot lately? Something feels off about how much data itâ€™s pulling.',\n",
    "  'date': 1769594405,\n",
    "  'message_id': 0},\n",
    " {'user': 'sam',\n",
    "  'message': 'Yeah, I noticed that too. Itâ€™s not just metadata, itâ€™s actually reading message contents.',\n",
    "  'date': 1769594440,\n",
    "  'message_id': 1},\n",
    " {'user': 'alex',\n",
    "  'message': 'Exactly. That goes way beyond what they mention in their privacy policy.',\n",
    "  'date': 1769594475,\n",
    "  'message_id': 2},\n",
    " {'user': 'sam',\n",
    "  'message': 'The scary part is itâ€™s happening silently. No clear opt-in, no warning.',\n",
    "  'date': 1769594510,\n",
    "  'message_id': 3},\n",
    " {'user': 'alex',\n",
    "  'message': 'If this gets out, itâ€™s a massive trust violation. Almost spyware-level behavior.',\n",
    "  'date': 1769594550,\n",
    "  'message_id': 4},\n",
    " {'user': 'sam',\n",
    "  'message': 'Iâ€™m surprised more people arenâ€™t talking about it yet. This could blow up.',\n",
    "  'date': 1769594600,\n",
    "  'message_id': 5},\n",
    " {'user': 'alex',\n",
    "  'message': 'Switching gears â€” have you heard the new STT models that dropped last week?',\n",
    "  'date': 1769596205,\n",
    "  'message_id': 6},\n",
    " {'user': 'sam',\n",
    "  'message': 'Bro yes ðŸ˜‚ theyâ€™re scary good. ElevenLabs must be sweating right now.',\n",
    "  'date': 1769596250,\n",
    "  'message_id': 7},\n",
    " {'user': 'alex',\n",
    "  'message': 'For real. Latency is lower and accents are handled way better.',\n",
    "  'date': 1769596290,\n",
    "  'message_id': 8},\n",
    " {'user': 'sam',\n",
    "  'message': 'I tested one with background noise and it still nailed the transcript.',\n",
    "  'date': 1769596330,\n",
    "  'message_id': 9},\n",
    " {'user': 'alex',\n",
    "  'message': 'If pricing stays sane, this is going to eat a chunk of ElevenLabsâ€™ use cases.',\n",
    "  'date': 1769596370,\n",
    "  'message_id': 10},\n",
    " {'user': 'sam',\n",
    "  'message': 'ElevenLabs be like: adding â€˜emotional whisper v4â€™ wonâ€™t save us ðŸ’€',\n",
    "  'date': 1769596410,\n",
    "  'message_id': 11},\n",
    " {'user': 'alex',\n",
    "  'message': 'Haha seriously. Accuracy beats fancy voices any day.',\n",
    "  'date': 1769596450,\n",
    "  'message_id': 12},\n",
    " {'user': 'sam',\n",
    "  'message': 'Give it a month and every demo app will be using the new STT.',\n",
    "  'date': 1769596490,\n",
    "  'message_id': 13},\n",
    " {'user': 'alex',\n",
    "  'message': 'And every founder will say itâ€™s â€˜just an experimentâ€™. Classic.',\n",
    "  'date': 1769596530,\n",
    "  'message_id': 14},\n",
    " {'user': 'sam',\n",
    "  'message': 'Meanwhile ElevenLabs marketing team pulling all-nighters.',\n",
    "  'date': 1769596555,\n",
    "  'message_id': 15},\n",
    " {'user': 'alex',\n",
    "  'message': 'Random question â€” whatâ€™s your max deadlift these days?',\n",
    "  'date': 1769601605,\n",
    "  'message_id': 16},\n",
    " {'user': 'sam',\n",
    "  'message': '120 lbs for reps. Not huge, but I can do like 12 clean.',\n",
    "  'date': 1769601650,\n",
    "  'message_id': 17},\n",
    " {'user': 'alex',\n",
    "  'message': '12 reps at 120 is solid. Form still tight on the last few?',\n",
    "  'date': 1769601690,\n",
    "  'message_id': 18},\n",
    " {'user': 'sam',\n",
    "  'message': 'Yeah, last 2 are a grind but no rounding. Lower back feels fine.',\n",
    "  'date': 1769601725,\n",
    "  'message_id': 19}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geimini_2_5_flash = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    ")\n",
    "\n",
    "open_ai_4_0_mini = init_chat_model('gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkMessageOutput(TypedDict):\n",
    "  topic: Annotated[str, 'Topic of the conversations it is about']\n",
    "  summary: Annotated[str, 'Summary of this chunk of messages']\n",
    "  message_ids: Annotated[list[int], 'List of ids for this conversations']\n",
    "  is_content_worthy: Annotated[bool, 'True if we can make a social media content about this chunk or not']\n",
    "\n",
    "class ChunkingMessagesOutput(TypedDict):\n",
    "  chunks: Annotated[list[ChunkMessageOutput], 'List of all the chunks made from past conversations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking_llm = geimini_2_5_flash.with_structured_output(ChunkingMessagesOutput)\n",
    "chunking_llm = open_ai_4_0_mini.with_structured_output(ChunkingMessagesOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chunking_llm.invoke([\n",
    "    SystemMessage(content=\"\"\"Analyze these Slack messages and group them by conversation topic.\n",
    "\n",
    "For each chunk, provide:\n",
    "1. A clear topic name\n",
    "2. A concise summary of the conversation\n",
    "3. The message IDs included in this chunk\n",
    "4. Content-worthiness evaluation: Set is_content_worthy to True only if the conversation:\n",
    "   - Contains interesting insights, opinions, or discussions\n",
    "   - Would be valuable/engaging for social media audiences\n",
    "   - Has substance beyond casual small talk\n",
    "   - Could generate meaningful engagement (likes, comments, shares)\n",
    "   \n",
    "   Set to False for:\n",
    "   - Personal conversations (e.g., gym routines, personal questions)\n",
    "   - Technical debugging or internal processes\n",
    "   - Casual banter without insights\n",
    "   - Sensitive or private topics\"\"\"),\n",
    "    HumanMessage(\n",
    "        content=json.dumps(conversations, indent=2)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7213b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_with_chunk_id = [{'chunk_id': i, **chunk} for i, chunk in enumerate(res['chunks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_with_chunk_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90899f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGenerationOutput(TypedDict):\n",
    "  hook: Annotated[str, 'Puchy hook very first line for the twitter post.']\n",
    "  tweets: Annotated[list[str], 'List of one or many tweets on this topic.']\n",
    "  \n",
    "# x_generation_llm = geimini_2_5_flash.with_structured_output(XGenerationOutput)\n",
    "x_generation_llm = open_ai_4_0_mini.with_structured_output(XGenerationOutput)\n",
    "\n",
    "\n",
    "class LinkedinGenerationOutput(TypedDict):\n",
    "  content: Annotated[str, 'Full LinkedIn post text (150-200 words)']\n",
    "  hook: Annotated[str, 'Opening line that grabs attention']\n",
    "  cta: Annotated[str, 'Call to action at the end of the post']\n",
    "  \n",
    "# class LinkedinGenerationOutput(TypedDict):\n",
    "#     content: str\n",
    "#     hook: str\n",
    "#     cta: str\n",
    "  \n",
    "# linkedin_generation_llm = geimini_2_5_flash.with_structured_output(LinkedinGenerationOutput)\n",
    "linkedin_generation_llm = open_ai_4_0_mini.with_structured_output(LinkedinGenerationOutput)\n",
    "\n",
    "class PostEvaluationOutput(TypedDict):\n",
    "  external_value_score: Annotated[int, 'Score 0-10: Useful to non-insiders?']\n",
    "  authenticity_score: Annotated[int, 'Score 0-10: Genuine vs corporate?']\n",
    "  clarity_score: Annotated[int, 'Score 0-10: Message clear?']\n",
    "  engagement_score: Annotated[int, 'Score 0-10: Would people interact?']\n",
    "  reasoning: Annotated[str, 'Explanation of the scores and evaluation']\n",
    "\n",
    "# evaluation_llm = geimini_2_5_flash.with_structured_output(PostEvaluationOutput)\n",
    "evaluation_llm = open_ai_4_0_mini.with_structured_output(PostEvaluationOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d844be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 0th chunk (first chunk)\n",
    "selected_chunk = res['chunks'][0] if res['chunks'] else None\n",
    "\n",
    "if selected_chunk is None:\n",
    "    raise ValueError(\"No chunks found in the result\")\n",
    "\n",
    "print(f\"Selected chunk topic: {selected_chunk['topic']}\")\n",
    "print(f\"Message IDs: {selected_chunk['message_ids']}\")\n",
    "\n",
    "# Filter conversations by message_ids\n",
    "chunk_messages = [msg for msg in conversations if msg['message_id'] in selected_chunk['message_ids']]\n",
    "\n",
    "print(f\"\\nFound {len(chunk_messages)} messages for this chunk:\")\n",
    "for msg in chunk_messages:\n",
    "    print(f\"  [{msg['user']}]: {msg['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a54f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tweets from the chunk messages\n",
    "x_system_prompt = \"\"\"You are a tech founder creating Twitter/X content from internal team conversations.\n",
    "\n",
    "Your task: Transform the provided Slack conversation into engaging Twitter/X posts that share tactical insights and authentic founder perspectives.\n",
    "\n",
    "OUTPUT STRUCTURE (you must provide both fields):\n",
    "1. `hook`: A punchy, attention-grabbing first line (1-2 sentences max). This is the opener that makes people stop scrolling. Use questions, bold statements, or surprising insights.\n",
    "2. `tweets`: A list of 1-3 tweets (strings) that follow the hook. These should form a cohesive thread with tactical insights and clear takeaways.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Generate 1-3 tweets maximum in the `tweets` list (use multiple only if the topic needs a thread)\n",
    "- The `hook` must be strong and grab attention immediately\n",
    "- Write in an authentic founder voice (conversational, insightful, not corporate)\n",
    "- Include punchy, tactical insights that provide real value\n",
    "- Each tweet should end with a clear takeaway that readers can act on\n",
    "- Keep tweets concise and impactful (under 280 characters each)\n",
    "- Use natural language, avoid jargon unless necessary\n",
    "- Make it feel like a real founder sharing learnings, not marketing copy\n",
    "\n",
    "TONE:\n",
    "- Direct and honest\n",
    "- Slightly casual but professional\n",
    "- Thoughtful and insightful\n",
    "- Engaging and relatable\n",
    "\n",
    "STRUCTURE EXAMPLE:\n",
    "hook: \"Just tested the new STT models. ElevenLabs should be worried.\"\n",
    "tweets: [\n",
    "  \"Lower latency + better accent handling = game changer for voice apps.\",\n",
    "  \"Accuracy > fancy features. Every time.\"\n",
    "]\n",
    "\n",
    "Now transform the conversation below into Twitter/X posts following these guidelines. Provide both the hook and the tweets list.\"\"\"\n",
    "\n",
    "x_human_prompt = f\"\"\"Topic: {selected_chunk['topic']}\n",
    "Summary: {selected_chunk['summary']}\n",
    "\n",
    "Original conversation messages:\n",
    "{json.dumps(chunk_messages, indent=2)}\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"X (TWITTER) GENERATION PROMPT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n[SYSTEM MESSAGE]\")\n",
    "print(x_system_prompt)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"[HUMAN MESSAGE]\")\n",
    "print(x_human_prompt)\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "tweet_result = x_generation_llm.invoke([\n",
    "    SystemMessage(content=x_system_prompt),\n",
    "    HumanMessage(content=x_human_prompt)\n",
    "])\n",
    "\n",
    "print(\"Generated Tweets:\")\n",
    "print(f\"\\nHook: {tweet_result['hook']}\\n\")\n",
    "for i, tweet in enumerate(tweet_result['tweets'], 1):\n",
    "    print(f\"Tweet {i}: {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a08077",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_system_prompt = \"\"\"You are a tech founder creating LinkedIn content from internal team conversations.\n",
    "\n",
    "Transform the Slack conversation into a professional LinkedIn post (150-200 words) with storytelling tone.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- 150-200 words for content field\n",
    "- Professional storytelling tone\n",
    "- Strong hook that captures attention\n",
    "- Practical insights/lessons readers can apply\n",
    "- Clear CTA at the end\n",
    "- Authentic founder voice (professional but relatable)\n",
    "\n",
    "TONE: Professional, conversational, thoughtful, engaging, authentic.\"\"\"\n",
    "\n",
    "linkedin_human_prompt = f\"\"\"Topic: {selected_chunk['topic']}\n",
    "Summary: {selected_chunk['summary']}\n",
    "\n",
    "Original conversation messages:\n",
    "{json.dumps(chunk_messages, indent=2)}\"\"\"\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"LINKEDIN GENERATION PROMPT\")\n",
    "# print(\"=\" * 80)\n",
    "# print(\"\\n[SYSTEM MESSAGE]\")\n",
    "# print(linkedin_system_prompt)\n",
    "# print(\"\\n\" + \"-\" * 80)\n",
    "# print(\"[HUMAN MESSAGE]\")\n",
    "# print(linkedin_human_prompt)\n",
    "# print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# print(\"Generating LinkedIn post (streaming)...\")\n",
    "# print(\"\\n\" + \"-\" * 80)\n",
    "# print(\"STREAMING OUTPUT:\")\n",
    "# print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=linkedin_system_prompt),\n",
    "    HumanMessage(content=linkedin_human_prompt)\n",
    "]\n",
    "\n",
    "\n",
    "linkedin_result = linkedin_generation_llm.invoke(messages)\n",
    "\n",
    "print(f\"Hook: {linkedin_result.get('hook', 'N/A')}\\n\")\n",
    "print(f\"Content:\\n{linkedin_result.get('content', 'N/A')}\\n\")\n",
    "print(f\"CTA: {linkedin_result.get('cta', 'N/A')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb43e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_post(post_data: dict, platform: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a social media post based on platform-specific criteria.\n",
    "    \n",
    "    Args:\n",
    "        post_data: Dictionary containing the post content (hook, tweets/content, cta)\n",
    "        platform: Either 'x' or 'linkedin'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation scores and reasoning\n",
    "    \"\"\"\n",
    "    # Base evaluation prompt\n",
    "    base_prompt = \"\"\"You are an expert social media content evaluator. Evaluate the provided post based on these criteria:\n",
    "\n",
    "SCORING CRITERIA (0-10 scale for each):\n",
    "1. External Value (0-10): Is this useful to non-insiders? Would someone outside the company/team find value?\n",
    "2. Authenticity (0-10): Does this feel genuine and authentic, or corporate/marketing-like?\n",
    "3. Clarity (0-10): Is the message clear and easy to understand?\n",
    "4. Engagement Potential (0-10): Would people want to interact (like, comment, share)?\n",
    "\n",
    "\n",
    "Provide detailed reasoning explaining each score.\"\"\"\n",
    "\n",
    "    # Platform-specific additions\n",
    "    if platform.lower() == 'x' or platform.lower() == 'twitter':\n",
    "        platform_specific = \"\"\"\n",
    "PLATFORM: Twitter/X\n",
    "\n",
    "SPECIFIC CHECKLIST:\n",
    "- Punchy, tactical insights: Does it provide actionable, tactical value?\n",
    "- Strong hook in first tweet: Does the hook grab attention immediately?\n",
    "- Authentic founder voice: Does it sound like a real founder, not corporate PR?\n",
    "- Clear takeaway: Is there a clear, actionable lesson or insight?\n",
    "\n",
    "Evaluate how well the post meets these Twitter/X-specific requirements.\"\"\"\n",
    "        \n",
    "        # Format post content for evaluation\n",
    "        hook = post_data.get('hook', '')\n",
    "        tweets = post_data.get('tweets', [])\n",
    "        post_content = f\"Hook: {hook}\\n\\nTweets:\\n\" + \"\\n\".join([f\"{i+1}. {tweet}\" for i, tweet in enumerate(tweets)])\n",
    "        \n",
    "    elif platform.lower() == 'linkedin':\n",
    "        platform_specific = \"\"\"\n",
    "PLATFORM: LinkedIn\n",
    "\n",
    "SPECIFIC CHECKLIST:\n",
    "- Professional storytelling tone: Is it polished but authentic?\n",
    "- Starts with a hook: Does it begin with an attention-grabbing opening?\n",
    "- Practical insight/lesson: Does it provide actionable insights readers can apply?\n",
    "- Clear call-to-action: Is there a clear CTA that encourages engagement?\n",
    "\n",
    "Evaluate how well the post meets these LinkedIn-specific requirements.\"\"\"\n",
    "        \n",
    "        # Format post content for evaluation\n",
    "        hook = post_data.get('hook', '')\n",
    "        content = post_data.get('content', '')\n",
    "        cta = post_data.get('cta', '')\n",
    "        post_content = f\"Hook: {hook}\\n\\nContent:\\n{content}\\n\\nCTA: {cta}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown platform: {platform}. Use 'x' or 'linkedin'\")\n",
    "    \n",
    "    # Combine prompts\n",
    "    full_prompt = base_prompt + \"\\n\\n\" + platform_specific\n",
    "    \n",
    "    # Create evaluation message\n",
    "    evaluation_message = f\"\"\"Post to evaluate:\n",
    "\n",
    "{post_content}\n",
    "\n",
    "Evaluate this post according to the criteria above.\"\"\"\n",
    "    \n",
    "    # Invoke evaluation LLM\n",
    "    result = evaluation_llm.invoke([\n",
    "        SystemMessage(content=full_prompt),\n",
    "        HumanMessage(content=evaluation_message)\n",
    "    ])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Evaluate X/Twitter post\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING X/TWITTER POST\")\n",
    "print(\"=\" * 80)\n",
    "x_evaluation = evaluate_post(tweet_result, 'x')\n",
    "print(f\"External Value: {x_evaluation['external_value_score']}/10\")\n",
    "print(f\"Authenticity: {x_evaluation['authenticity_score']}/10\")\n",
    "print(f\"Clarity: {x_evaluation['clarity_score']}/10\")\n",
    "print(f\"Engagement Potential: {x_evaluation['engagement_score']}/10\")\n",
    "print(f\"\\nReasoning:\\n{x_evaluation['reasoning']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING LINKEDIN POST\")\n",
    "print(\"=\" * 80)\n",
    "linkedin_evaluation = evaluate_post(linkedin_result, 'linkedin')\n",
    "print(f\"External Value: {linkedin_evaluation['external_value_score']}/10\")\n",
    "print(f\"Authenticity: {linkedin_evaluation['authenticity_score']}/10\")\n",
    "print(f\"Clarity: {linkedin_evaluation['clarity_score']}/10\")\n",
    "print(f\"Engagement Potential: {linkedin_evaluation['engagement_score']}/10\")\n",
    "print(f\"\\nReasoning:\\n{linkedin_evaluation['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3004e",
   "metadata": {},
   "source": [
    "# Time to roll langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from operator import add\n",
    "\n",
    "class GeneratedChunksOutput(TypedDict, ChunkMessageOutput):\n",
    "    chunk_id: int\n",
    "    \n",
    "class JobState(TypedDict):\n",
    "    chunk: GeneratedChunksOutput\n",
    "    platform: Literal['x', 'linkedin']\n",
    "    chunk_messages: list[dict]\n",
    "    \n",
    "    x_post: XGenerationOutput\n",
    "    linkedin_post: LinkedinGenerationOutput\n",
    "    evaluation: PostEvaluationOutput\n",
    "    evaluation_passed: bool\n",
    "    \n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "\n",
    "# Use a reducer for jobs to accumulate results from parallel processing\n",
    "class GlobalState(TypedDict):\n",
    "    messages: list[dict]\n",
    "    chunks: list[GeneratedChunksOutput]\n",
    "    jobs: Annotated[list[JobState], add]\n",
    "    jobs_result: Annotated[list[JobState], add]\n",
    "    max_iterations_per_job: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad84d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_messages(state: GlobalState) -> GlobalState:\n",
    "    res = chunking_llm.invoke([\n",
    "        SystemMessage(content=\"\"\"Analyze these Slack messages and group them by conversation topic.\n",
    "\n",
    "For each chunk, provide:\n",
    "1. A clear topic name\n",
    "2. A concise summary of the conversation\n",
    "3. The message IDs included in this chunk\n",
    "4. Content-worthiness evaluation: Set is_content_worthy to True only if the conversation:\n",
    "   - Contains interesting insights, opinions, or discussions\n",
    "   - Would be valuable/engaging for social media audiences\n",
    "   - Has substance beyond casual small talk\n",
    "   - Could generate meaningful engagement (likes, comments, shares)\n",
    "   \n",
    "   Set to False for:\n",
    "   - Personal conversations (e.g., gym routines, personal questions)\n",
    "   - Technical debugging or internal processes\n",
    "   - Casual banter without insights\n",
    "   - Sensitive or private topics\"\"\"),\n",
    "        HumanMessage(\n",
    "            content=json.dumps(state['messages'], indent=2)\n",
    "        )\n",
    "    ])\n",
    "    # add a chunk_id field\n",
    "    chunks = [{\"chunk_id\": i, **chunk}\n",
    "              for i, chunk in enumerate(res['chunks'])]\n",
    "    return {'chunks': chunks}\n",
    "\n",
    "\n",
    "def prepare_jobs(state: GlobalState):\n",
    "    \"\"\"Create jobs from chunks and store in state.\"\"\"\n",
    "    jobs = []\n",
    "\n",
    "    for chunk in state[\"chunks\"]:\n",
    "        chunk_messages = [\n",
    "            msg for msg in state['messages'] if msg['message_id'] in chunk['message_ids']]\n",
    "\n",
    "        for platform in (\"x\", \"linkedin\"):\n",
    "            job = {\n",
    "                \"chunk\": chunk,\n",
    "                \"platform\": platform,\n",
    "                \"chunk_messages\": chunk_messages,\n",
    "                \"iteration\": 0,\n",
    "                \"max_iterations\": state['max_iterations_per_job'],\n",
    "            }\n",
    "            jobs.append(job)\n",
    "\n",
    "    print(f\"Final jobs count: {len(jobs)}\")\n",
    "    return {\"jobs\": jobs}\n",
    "\n",
    "\n",
    "def continue_to_jobs(state: GlobalState):\n",
    "    \"\"\"Route each job to process_job node using Send.\"\"\"\n",
    "    return [Send(\"process_job\", job) for job in state[\"jobs\"]]\n",
    "\n",
    "\n",
    "def generate_x_post(state: JobState) -> JobState:\n",
    "    # support being passed the chunk and its messages separately\n",
    "    if not state['chunk']['is_content_worthy']:\n",
    "        return {'x_post': None}\n",
    "\n",
    "    x_system_prompt = \"\"\"You are a tech founder creating Twitter/X content from internal team conversations.\n",
    "\n",
    "Your task: Transform the provided Slack conversation into engaging Twitter/X posts that share tactical insights and authentic founder perspectives.\n",
    "\n",
    "OUTPUT STRUCTURE (you must provide both fields):\n",
    "1. `hook`: A punchy, attention-grabbing first line (1-2 sentences max). This is the opener that makes people stop scrolling. Use questions, bold statements, or surprising insights.\n",
    "2. `tweets`: A list of 1-3 tweets (strings) that follow the hook. These should form a cohesive thread with tactical insights and clear takeaways.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Generate 1-3 tweets maximum in the `tweets` list (use multiple only if the topic needs a thread)\n",
    "- The `hook` must be strong and grab attention immediately\n",
    "- Write in an authentic founder voice (conversational, insightful, not corporate)\n",
    "- Include punchy, tactical insights that provide real value\n",
    "- Each tweet should end with a clear takeaway that readers can act on\n",
    "- Keep tweets concise and impactful (under 280 characters each)\n",
    "- Use natural language, avoid jargon unless necessary\n",
    "- Make it feel like a real founder sharing learnings, not marketing copy\n",
    "\n",
    "TONE:\n",
    "- Direct and honest\n",
    "- Slightly casual but professional\n",
    "- Thoughtful and insightful\n",
    "- Engaging and relatable\n",
    "\n",
    "STRUCTURE EXAMPLE:\n",
    "hook: \"Just tested the new STT models. ElevenLabs should be worried.\"\n",
    "tweets: [\n",
    "  \"Lower latency + better accent handling = game changer for voice apps.\",\n",
    "  \"Accuracy > fancy features. Every time.\"\n",
    "]\n",
    "\n",
    "Now transform the conversation below into Twitter/X posts following these guidelines. Provide both the hook and the tweets list.\"\"\"\n",
    "\n",
    "    # if evaluation failed previously add the reasoning\n",
    "    reasoning = \"\"\n",
    "    if not state.get('evaluation_passed', True) and state.get('evaluation') is not None:\n",
    "        reasoning = f\"\"\"\n",
    "Previously Generated Content\n",
    "{json.dumps(state['x_post'])}\n",
    "\n",
    "Previous Evaluation Scores:\n",
    "External Value: {state['evaluation']['external_value_score']}/10\n",
    "Authenticity: {state['evaluation']['authenticity_score']}/10\n",
    "Clarity: {state['evaluation']['clarity_score']}/10\n",
    "Engagement Potential: {state['evaluation']['engagement_score']}/10 \n",
    "\n",
    "Reasoning: {state['evaluation']['reasoning']}\n",
    "\n",
    "\"\"\"\n",
    "    x_system_prompt += reasoning\n",
    "    x_human_prompt = f\"\"\"Topic: {state['chunk']['topic']}\n",
    "Summary: {state['chunk']['summary']}         \n",
    "\n",
    "Original conversation messages:\n",
    "{json.dumps(state['chunk_messages'], indent=2)}\"\"\"\n",
    "    x_post = x_generation_llm.invoke([  \n",
    "        SystemMessage(content=x_system_prompt),\n",
    "        HumanMessage(content=x_human_prompt)\n",
    "    ])\n",
    "\n",
    "    return x_post\n",
    "\n",
    "\n",
    "def generate_linkedin_post(state: JobState) -> JobState:\n",
    "    # support being passed the chunk and its messages separately\n",
    "    if not state['chunk']['is_content_worthy']:\n",
    "        return {'linkedin_post': None}\n",
    "\n",
    "    linkedin_system_prompt = \"\"\"You are a tech founder creating LinkedIn content from internal team conversations.\n",
    "\n",
    "Transform the Slack conversation into a professional LinkedIn post (150-200 words) with storytelling tone.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- 150-200 words for content field\n",
    "- Professional storytelling tone\n",
    "- Strong hook that captures attention\n",
    "- Practical insights/lessons readers can apply\n",
    "- Clear CTA at the end\n",
    "- Authentic founder voice (professional but relatable)\n",
    "\n",
    "TONE: Professional, conversational, thoughtful, engaging, authentic.\"\"\"\n",
    "\n",
    "    # if evaluation failed previously add the reasoning\n",
    "    reasoning = \"\"\n",
    "    if not state.get('evaluation_passed', True) and state.get('evaluation') is not None:\n",
    "        reasoning = f\"\"\"\n",
    "Previously Generated Content\n",
    "{json.dumps(state['linkedin_post'])}\n",
    "\n",
    "Previous Evaluation Scores:\n",
    "External Value: {state['evaluation']['external_value_score']}/10\n",
    "Authenticity: {state['evaluation']['authenticity_score']}/10\n",
    "Clarity: {state['evaluation']['clarity_score']}/10\n",
    "Engagement Potential: {state['evaluation']['engagement_score']}/10 \n",
    "\n",
    "Reasoning: {state['evaluation']['reasoning']}\n",
    "\n",
    "\"\"\"\n",
    "    linkedin_system_prompt += reasoning\n",
    "    \n",
    "    linkedin_human_prompt = f\"\"\"Topic: {state['chunk']['topic']}\n",
    "Summary: {state['chunk']['summary']}\n",
    "\n",
    "Original conversation messages:\n",
    "{json.dumps(state['chunk_messages'], indent=2)}\"\"\"\n",
    "    \n",
    "    linkedin_post = linkedin_generation_llm.invoke([\n",
    "        SystemMessage(content=linkedin_system_prompt),\n",
    "        HumanMessage(content=linkedin_human_prompt)\n",
    "    ])\n",
    "\n",
    "    return linkedin_post\n",
    "\n",
    "\n",
    "def generate_post(state: JobState) -> JobState:\n",
    "    if not state['chunk']['is_content_worthy']:\n",
    "        return {}\n",
    "\n",
    "    if state['platform'] == 'x':\n",
    "        res = generate_x_post(state)\n",
    "        return {'x_post': res, 'iteration': state['iteration'] + 1}\n",
    "    elif state['platform'] == 'linkedin':\n",
    "        res = generate_linkedin_post(state)\n",
    "        return {'linkedin_post': res, 'iteration': state['iteration'] + 1}\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown platform: {state['platform']}\")\n",
    "\n",
    "\n",
    "def evaluate_post(state: JobState) -> JobState:\n",
    "    # if no post was generated, skip evaluation\n",
    "    if state['platform'] == 'x' and state.get('x_post') is None:\n",
    "        return {'evaluation': None, 'evaluation_passed': True}\n",
    "    if state['platform'] == 'linkedin' and state.get('linkedin_post') is None:\n",
    "        return {'evaluation': None, 'evaluation_passed': True}\n",
    "\n",
    "    platform = state['platform']\n",
    "\n",
    "    # Base evaluation prompt\n",
    "    base_prompt = \"\"\"You are an expert social media content evaluator. Evaluate the provided post based on these criteria:\n",
    "\n",
    "SCORING CRITERIA (0-10 scale for each):\n",
    "1. External Value (0-10): Is this useful to non-insiders? Would someone outside the company/team find value?\n",
    "2. Authenticity (0-10): Does this feel genuine and authentic, or corporate/marketing-like?\n",
    "3. Clarity (0-10): Is the message clear and easy to understand?\n",
    "4. Engagement Potential (0-10): Would people want to interact (like, comment, share)?\n",
    "\n",
    "\n",
    "Provide detailed reasoning explaining each score.\"\"\"\n",
    "\n",
    "    # Platform-specific additions\n",
    "    if platform == 'x':\n",
    "        platform_specific = \"\"\"\n",
    "PLATFORM: Twitter/X\n",
    "\n",
    "SPECIFIC CHECKLIST:\n",
    "- Punchy, tactical insights: Does it provide actionable, tactical value?\n",
    "- Strong hook in first tweet: Does the hook grab attention immediately?\n",
    "- Authentic founder voice: Does it sound like a real founder, not corporate PR?\n",
    "- Clear takeaway: Is there a clear, actionable lesson or insight?\n",
    "\n",
    "Evaluate how well the post meets these Twitter/X-specific requirements.\"\"\"\n",
    "\n",
    "        # Format post content for evaluation\n",
    "        hook = state['x_post'].get('hook', '')\n",
    "        tweets = state['x_post'].get('tweets', [])\n",
    "        post_content = f\"Hook: {hook}\\n\\nTweets:\\n\" + \\\n",
    "            \"\\n\".join([f\"{i+1}. {tweet}\" for i, tweet in enumerate(tweets)])\n",
    "\n",
    "    elif platform == 'linkedin':\n",
    "        platform_specific = \"\"\"\n",
    "PLATFORM: LinkedIn\n",
    "\n",
    "SPECIFIC CHECKLIST:\n",
    "- Professional storytelling tone: Is it polished but authentic?\n",
    "- Starts with a hook: Does it begin with an attention-grabbing opening?\n",
    "- Practical insight/lesson: Does it provide actionable insights readers can apply?\n",
    "- Clear call-to-action: Is there a clear CTA that encourages engagement?\n",
    "\n",
    "Evaluate how well the post meets these LinkedIn-specific requirements.\"\"\"\n",
    "\n",
    "        # Format post content for evaluation\n",
    "        hook = state['linkedin_post'].get('hook', '')\n",
    "        content = state['linkedin_post'].get('content', '')\n",
    "        cta = state['linkedin_post'].get('cta', '')\n",
    "        post_content = f\"Hook: {hook}\\n\\nContent:\\n{content}\\n\\nCTA: {cta}\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unknown platform: {state['platform']}. Use 'x' or 'linkedin'\")\n",
    "\n",
    "    # Combine prompts\n",
    "    full_prompt = base_prompt + \"\\n\\n\" + platform_specific\n",
    "\n",
    "    # Create evaluation message\n",
    "    evaluation_message = f\"\"\"Post to evaluate:\n",
    "\n",
    "{post_content}\n",
    "\n",
    "Evaluate this post according to the criteria above.\"\"\"\n",
    "\n",
    "    # Invoke evaluation LLM\n",
    "    result = evaluation_llm.invoke([\n",
    "        SystemMessage(content=full_prompt),\n",
    "        HumanMessage(content=evaluation_message)\n",
    "    ])\n",
    "\n",
    "    total_score = (\n",
    "        result['external_value_score'] +\n",
    "        result['authenticity_score'] +\n",
    "        result['clarity_score'] +\n",
    "        result['engagement_score']\n",
    "    )\n",
    "    return {'evaluation': result, 'evaluation_passed': True if total_score >= 30 else False}\n",
    "\n",
    "\n",
    "def re_try_generation(state: JobState) -> Literal['end', 'retry']:\n",
    "    if state['evaluation_passed'] or state['iteration'] >= state['max_iterations']:\n",
    "        return 'end'\n",
    "    return 'retry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "job_graph = StateGraph(JobState)\n",
    "\n",
    "job_graph.add_node(\"generate_post\", generate_post)\n",
    "job_graph.add_node(\"evaluate_post\", evaluate_post)\n",
    "\n",
    "job_graph.add_edge(START, \"generate_post\")\n",
    "job_graph.add_edge(\"generate_post\", \"evaluate_post\")\n",
    "job_graph.add_conditional_edges(\"evaluate_post\", re_try_generation, {\"end\": END, \"retry\": \"generate_post\"})\n",
    "\n",
    "# job_graph.add_conditional_edges(\n",
    "#     \"evaluate\",\n",
    "#     route_after_eval,\n",
    "#     {\n",
    "#         \"generate\": \"generate\",\n",
    "#         \"end\": END\n",
    "#     }\n",
    "# )\n",
    "\n",
    "job_subgraph = job_graph.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(job_subgraph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cf906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs = prepare_jobs({'messages': conversations, 'max_iterations_per_job': 3, 'chunks': res_with_chunk_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = job_subgraph.invoke(jobs['jobs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Graph\n",
    "def process_job(job: JobState):\n",
    "    \"\"\"Wrapper function to process a single job through the job subgraph and return result.\"\"\"\n",
    "    print(f\"\\n=== process_job called for {job['chunk']['topic']} ===\")\n",
    "    result = job_subgraph.invoke(job)\n",
    "    print(f\"=== process_job completed ===\\n\")\n",
    "    # Return as list to be added to jobs via reducer\n",
    "    return {'jobs_result': [result]}\n",
    "\n",
    "graph = StateGraph(GlobalState)\n",
    "graph.add_node(\"chunk_messages\", chunk_messages)\n",
    "graph.add_node(\"prepare_jobs\", prepare_jobs)\n",
    "graph.add_node(\"process_job\", process_job)\n",
    "\n",
    "graph.add_edge(START, \"chunk_messages\")\n",
    "graph.add_edge(\"chunk_messages\", \"prepare_jobs\")\n",
    "# Use add_conditional_edges with continue_to_jobs routing function for Send API\n",
    "graph.add_conditional_edges(\"prepare_jobs\", continue_to_jobs, [\"process_job\"])\n",
    "graph.add_edge(\"process_job\", END)\n",
    "\n",
    "final_graph = graph.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(final_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11311aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = final_graph.invoke({\n",
    "    'messages': conversations,\n",
    "    'max_iterations_per_job': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb56555",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['jobs_result']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influencer-aunty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
